{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from preprocess import preprocess\n",
    "from nltk import bigrams, trigrams\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['RT', 'rt', 'via', 'http', 'https', ':/']\n",
    "\n",
    "print \"punctuation\", punctuation\n",
    " \n",
    "fname = 'brexit.json'\n",
    "with open(fname, 'r') as f:\n",
    "    # freq counters\n",
    "    count_all = Counter()\n",
    "    count_stop = Counter()\n",
    "    count_single = Counter()\n",
    "    count_hash = Counter()\n",
    "    count_term = Counter()\n",
    "    # n-gram counters\n",
    "    stop_bigram = Counter()\n",
    "    term_trigram = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "\n",
    "        tweet['text'] = tweet['text'].encode('utf-8').decode('unicode_escape').encode('ascii','ignore')\n",
    "        # Create a list with all the terms\n",
    "        terms_all = [term for term in preprocess(tweet['text'])]\n",
    "        count_all.update(terms_all)\n",
    "        # Create a list with all the terms (no stop words)\n",
    "        terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]\n",
    "        count_stop.update(terms_stop)\n",
    "        # Count terms only once, equivalent to Document Frequency\n",
    "        terms_single = set(terms_all)\n",
    "        count_single.update(terms_single)\n",
    "        # Count hashtags only\n",
    "        terms_hash = [term for term in preprocess(tweet['text']) \n",
    "                      if term.startswith('#')]\n",
    "        count_hash.update(terms_hash)\n",
    "        # Count terms only (no hashtags, no mentions)\n",
    "        terms_only = [term for term in preprocess(tweet['text']) \n",
    "                      if term not in stop and\n",
    "                      not term.startswith(('#', '@'))] \n",
    "                      # mind the ((double brackets))\n",
    "                      # startswith() takes a tuple (not a list) if \n",
    "                      # we pass a list of inputs\n",
    "        count_term.update(terms_only)\n",
    "        # n-grams\n",
    "        terms_bigram = bigrams(terms_stop)\n",
    "        stop_bigram.update(terms_bigram)\n",
    "        terms_trigram = trigrams(terms_all)\n",
    "        term_trigram.update(terms_trigram)\n",
    "        \n",
    "    # Print the first 5 most frequent words\n",
    "    print(\"Term frequencies\")\n",
    "    print(\"All: \", count_all.most_common(10))\n",
    "    print(\"Stop: \", count_stop.most_common(10))\n",
    "    print(\"Single: \", count_single.most_common(10))\n",
    "    print(\"Hash: \", count_hash.most_common(10))\n",
    "    print(\"Term only: \", count_term.most_common(10))\n",
    "    print(\"N-gram frequencies\")\n",
    "    print(\"Stop bigrams: \", stop_bigram.most_common(10))\n",
    "    print(\"Term trigrams: \", term_trigram.most_common(10))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from preprocess import preprocess\n",
    "from nltk import bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['RT', 'rt', 'via', 'http', 'https', ':/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets/2016-04-03.json\n",
      "tweets/2016-04-04.json\n",
      "tweets/2016-04-05.json\n",
      "tweets/2016-04-06.json\n",
      "tweets/2016-04-07.json\n",
      "tweets/2016-04-08.json\n",
      "tweets/2016-04-09.json\n",
      "tweets/2016-04-10.json\n",
      "tweets/2016-04-11.json\n",
      "tweets/2016-04-12.json\n"
     ]
    }
   ],
   "source": [
    " # term counters\n",
    "count_terms = Counter()\n",
    "count_vocab = Counter()\n",
    "count_no_stop = Counter()\n",
    "count_words = Counter()\n",
    "count_hashtag = Counter()\n",
    "count_mention = Counter()\n",
    "# n-gram counters\n",
    "stop_bigram = Counter()\n",
    "term_trigram = Counter()\n",
    "#files list\n",
    "data_list = ['tweets/2016-04-'+str(x)+'.json' for x in ['03', '04', '05', '06', '07', '08', '09', '10', '11', '12']] \n",
    "# loop through files\n",
    "for data in data_list:\n",
    "    # read json\n",
    "    with open(data, 'r') as f:\n",
    "        data_json = json.loads(f.read())\n",
    "    # loop tweets\n",
    "    for tweet in data_json:\n",
    "        # encode and preprocess\n",
    "        text = tweet['text'].encode('utf-8').decode('unicode_escape').encode('ascii','ignore')\n",
    "        terms = [term for term in preprocess(text)]\n",
    "        # filter\n",
    "        vocab = set(terms)\n",
    "        no_stop = [term for term in terms if not in stop]\n",
    "        words = [term for term in no_stop if not term.startswith(('#', '@'))]\n",
    "        hashtag = [term for term in terms if term.startswith('#')]\n",
    "        mention = [term for term in terms if term.startswith('#')]\n",
    "        # n-gram\n",
    "        bigram = bigrams(terms_stop)\n",
    "        trigram = trigrams(terms_stop)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
